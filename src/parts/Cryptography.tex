% !TeX root = ../index.tex
\section{Cryptography}

Cryptography is a field of mathematics that uses computationally difficult problems to obfuscate, hide, split and verify data. Starting in the ancient times with the famous Caesar cipher, which relied on shifting the letters in a message in coordination to an alphabet, there are multiple cryptographic protocols today for a variety of use cases, which are used in both public and private messaging. As mentioned earlier, while the familiar connotation of cryptography as a word brings secrecy and secure communications into mind, it can also be used to verify data, and to prove that something has happened. I will go over cryptography subjects that are relevant to the Proof of Latency protocol in this chapter, like groups of unknown order, RSA and cryptographic proofs in general.

Modern cryptography is based on relatively few mathematical fundamentals. The most dominant one during the age of computers has been prime numbers with modular multiplication, but elliptic curve cryptography is used widely today because of its easy and less resource-intensive key generation method when compared to the prime number based RSA cryptosystem. Both RSA and elliptic curve cryptography use modular arithmetic in their operations.

Modular\footnote{Using the modulus operation; taking the remainder of a division. Useful in cryptography because it creates finite cyclical groups.} arithmetic with large composite semiprimes has a useful property that checking a factorization is quick but finding the two large prime factors is a hard problem. This is called the discrete logarithm problem.~\cite{Joux2014-rz} With public key cryptography, you can exchange encrypted data without the participants knowing each other's private keys, but requiring a computation that is theoretically almost impossible to break due to discrete logarithm problem. These large numbers that are used as the modulo need to have unknown factorizations to create groups of unknown order, that are paramount to cryptographic systems that rely on the factorization problem for their safety. The order of a group means the amount of elements in it. Cryptographic proofs need to be created with groups of unknown orders for them to be valid, as knowing the order of a group means that the modulo used in the cryptographic operations is so small that it has a known factorization, and the security of the factorization problem is broken.

\subsection{Cryptographic Proofs and Their Soundness}
Cryptographic proofs are proofs that depend on the trapdoor nature of cryptographic functions. In other words, cryptographic proofs are arguments that can be trusted if the cryptographic assumptions they depend on are valid and not broken.

The most well-known proofs, which are not necessarily thought of as such, are digital signatures. Given a public key, a prover in possession of its corresponding private key can produce a signature of any given data, given to a verifier together with the original data, that the prover has seen and processed the data. If the private key pertaining to the public key gets leaked to a third party, its security is broken, and the unique identity is lost.

Proofs can be private or public. A cryptographic proof can be categorized as public, if a verifier can gather all information required to verify the proof from the transcript of the proof itself, and verify the proof to be correct. Now, since classical cryptography is based on the fact that a computation is asymmetric --- being harder to compute the other way around, a cryptographic proof is still probabilistic in nature, and the security of it based not only on the algorithm but also the parameters used.

Proofs need to be tested for soundness to be called as such. Soundness means that a prover cannot make a verifier accept a false statement, except from a really small probability. This means that a proof is dependent on context, the verifier. If a proof holds statistically, it is a proof. If it instead holds only computationally, it should be instead called an argument~\cite{Bitansky2012-uz}.

\subsection{RSA}
RSA~\cite{Rivest1978-fm} is named after its discoverers â€“ Rivest, Shamir, and Adleman. It is an asymmetric public-key cryptosystem, meaning that it is based on a keypair, in which one is a public and the other is a secret. The public key is used to encrypt, and the secret key is used to decrypt. Everyone who has the public key can encrypt data and that encrypted data is practically impossible to decrypt without the secret key. This works due to the fact that it is hard to factor the product of two large prime numbers. RSA has been the most widely used cryptosystem since its creation, and is easy to make more secure by just increasing the size of the keys.

RSA uses an arithmetic trick that involves a mathematical object called a trapdoor permutation. Trapdoor permutation is a function that transforms a number \(x\) to a number \(y\) in the same range, in a way that computing \(y\) from \(x\) is easy using the public key but computing \(x\) from \(y\) is practically impossible without knowing the private key. The private key is the trapdoor~\cite{Aumasson2018-nh}. RSA relies on the prime factorization problem and the strong RSA assumption for its security. Strong RSA groups~\cite{Rivest2005-kc} are groups of unknown order, meaning that the number of elements in the group is unknown. RSA has played a big part in the development of cryptography, and even served as a motivation for the creation of verifiable delay functions, as it was used in the creation of time-lock puzzles closely related to VDFs~\cite{Boneh2018-sm}.

An RSA group, like other modular multiplicative groups, are commutative under their group operation, multiplication. This enables multiple interactive protocols and proofs, one of them being Diffie-Hellman key exchange. Diffie-Hellman key exchange enables two peers to create a shared key with their private keys in a modular multiplicative group without revealing their individual secrets because of the commutative property; order of operations do not matter. An example run of Diffie-Hellman key exchange goes as follows:

\begin{align*}
	A = 256^{1097} \mod 311\\
  B = 256^{1091} \mod 311\\
	A^B \equiv B^A \mod 311\\
\end{align*}

\subsubsection{RSA Setup}
When using RSA, the users need to setup the public parameters, which need to be distributed between the involved peers to be used in encryption. This key generation process is quite resource intensive, and is rarely used in ephemeral contexts, unless you can skip the key generation process, or ceremony, and use a publicly available modulo that is created by a trusted party. These types of key generation processes are called trusted setups, and they produce so-called toxic waste that needs to be discarded to preserve the key safety. To skip the trusted setup and thus the "toxic waste", one can use RSA numbers published by a trusted third party. This is sometimes even used in production systems, but it is highly encouraged to do a trusted setup in cases where security is the utmost requirement. In this thesis and the Proof of Latency protocol's proof of concept I use RSA-2048 for brevity.

\subsubsection{Public RSA Semiprimes}
The company formed by the RSA algorithm's inventors, RSA Security LLC, published public RSA semiprimes\footnote{A product of two prime numbers}, that are called RSA numbers, as a part of the RSA factoring challenge, from 1991 up until 2007 when the challenge ended~\cite{RSA_Laboratories2013-zk}. The challenge was ended because it had reached its purpose of forwarding science and understanding of common symmetric-key and public-key algorithms. Despite this, still under a half of the RSA numbers have been factored, and the largest of them might take hundreds if not even thousands of years to break even when given extraordinary hardware to do it with.
Using these factoring challenge RSA numbers requires the user to assume that nobody has access to the parameters. Given that the RSA numbers are said to have been created with a machine that was completely destroyed after their creation and the primes were never apparent to anyone, you need to trust the company's claims if you ever use these public challenge numbers. Fortunately, usually they are not used for encrypting large swathes of personal data, but in a more ephemeral context like Proof of Latency, which wouldn't cause a huge stir if the cryptosystem was broken and needed changing. Also, groups of unknown order work similarly to each other, so in most cases they are relatively interchangeable if not for the trusted setup.

\subsection{RSA Accumulators}
An RSA accumulator is a set of elements built from cryptographic assumptions in groups of unknown order, like \(\mathbb{Z}^*_N\)\footnote{Multiplicative group of integers modulo N}. It enables a prover to prove to a verifier that an element belongs or does not belong to a set by providing a succinct digest together with a proof~\cite{Tomescu2020-wq, Konstantopoulos2019-ls}. The digest is the latest state of the accumulator, and can thus be called the accumulator as well. This digest can be used to prove any previously accumulated elements' membership, or any non-membership. An interesting fact is that whenever an accumulator gets accumulated to, the latest accumulation is actually a binding commitment. This means that once an element has been accumulated to the set together with a proof, the proof is valid with every subsequent digest. This can't be changed unless the prover is able to roll back the accumulator to the state before the accumulation in question. In a P2P environment where proofs get broadcast and/or saved on multiple machines together with digital signatures done by the prover this is highly unlikely, so once an element gets accumulated, it has provably influenced the accumulator.

For the scheme to work, an RSA accumulator needs a function that can hash arbitrary input to a prime number. This is because every accumulated element needs to be unique for the proofs to work. In terms of code, this roughly means creating random odd numbers as fast as you can and taking a primality check of them until a prime number has been generated. This may sound bad performance-wise, but with modern processors and multithreading the problem isn't as hard as it might seem, as guesswork can be highly parallellized. The generated prime \(l\) needs to be larger than the modulus \(N\) of the accumulated set, \(l \notin \mathbb{Z}^*_N\), for the proofs to be secure.

\subsection{Commitment Schemes}
Commitment schemes are a way to lock causality in a cryptographic protocol~\cite{Damgard1998-vs}, and are among the most used primitives in cryptography. Commitments usually come in the form of commit-reveal, when at a later stage of the protocol the committer reveals the original message \(m\) they made their commitment \(C_m\) on. This has a great property --- a committer cannot change their committed message after revealing their commitment to the verifier, because the commitment would not hold for a new message. Commitment schemes are usually used when an interactive protocol requires for a message to be hidden until the termination of the protocol.

The requirements for a commitment scheme are that it is hiding and binding. Hiding means that nothing can be known of the committed message by looking at the commitment, not until or if the committer reveals it. Binding means that once a commitment has been made, the committer cannot get the same commitment with different data--- the commitment binds the committer to reveal the exact committed data they made the commitment with.

The function that is used to transform a message to a commitment could, for example, be a cryptographically secure hash function. In this case, a commit-reveal scheme works as follows:

\begin{enumerate}
  \item Committer hashes the message \(m\) with a hash function \(H(m)\)
  \item Committer publishes the hash \(H(m)\)
  \item Third parties see the commitment, meaning that the committer cannot change the message and convince the third parties with another input, locking its decision without revealing \(m\).
  \item Committer publishes the original message \(m\), convincing the third parties that the commitment was correct.
\end{enumerate}

\subsubsection{Vector Commitments}
Vector commitments are a cryptographic primitive formalized in 2013 consisting of a vector commitment \(com\) and openings to the vector.~\cite{Catalano2013-jn} Vector commitments are related to cryptographic accumulators, but add a position binding commitment to them. This means that once a opening has been made at an index, no other opening can reside at that given index.

The simplest form of a vector commitment (and a cryptographic accumulator!) is a merkle tree. Unfortunately, merkle proof size grows with input, and verifiers need to know the history instead of a concise digest of an accumulator. For example, Catalano-Fiore vector commitments improve upon the proof size, introducing a fixed size proof. Catalano-Fiore vector commitments are called concise, which means that the proof is at most a fixed size.~\cite{Catalano2013-jn} In addition to the fixed size, the verifier only needs to know the proof and the digest\footnote{latest output} to verify the proof. In layman's terms, vector commitments enable a prover to prove that an opening \(pi\) to the vector commitment \(com\) has been made at the index \(i\).

\subsubsection{Formal Definition}
Vector commitments are a tuple of four algorithms: VC.Setup, VC.Com, VC.Open, VC.Verify.~\cite{Boneh2019-tk}

\begin{enumerate}

  \item VC.Setup(\(\lambda\), n, M) \(\rightarrow\) \(pp\quad\) Given security parameter \(\lambda\), length \(n\) of the vector, and message space of vector components \(M\), output public parameters \(pp\), which are implicit inputs to all the following algorithms

  \item VC.Com(\(m\)) \(\rightarrow\) \(\tau, com\quad\) Given an input \(m\) = \((m_1, ..., m_n)\) output a commitment \(com\) and advice \(\tau\).

  \item VC.Open(\(com, m, i, \tau\)) \(\rightarrow\) \(\pi\quad\) On input \(m \in M\) and \(i \in [1, n]\), the commitment \(com\), and advice \(\tau\) output an opening \(\pi\) that proves \(m\) is the \(i\)th committed element of \(com\).

  \item VC.Verify(\(com, m, i, \pi\)) \(\rightarrow\) \(0/1\quad\) On input commitment \(com\), an index \(i \in [n]\), and an opening proof \(\pi\) output \(1\) (accept) or \(0\) (reject).

\end{enumerate}

Like regular commitment schemes, vector commitments are used to enforce causality. While regular commitment schemes make sure that a user of a protocol can't change an input or "skip the line" after the start, vector commitments create a causal relationship for multiple proofs. To wrap it up, if proofs are based on vector commitments, the commitments guarantee that the proofs' order can't change, unless the whole vector with all associated proofs are unvalidated.

