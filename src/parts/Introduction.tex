% !TeX root = ../index.tex
\chapter{Introduction}
\label{Introduction}

Networked computer applications, whether they are on the public Internet or on a private network, commonly use the client-server model of communication. In this model, the client application sends requests to the server, and the server responds. Applications are today, however, increasingly moving towards a distributed, peer-to-peer (P2P) networked model, where every peer, whether it is an entire computer or merely a process running on one, serves equally as the both sides of the client-server model. Many uses of P2P are not visible to the end user, because in many cases, P2P technologies serve to optimize resource utilization rather than as centerpoints of applications. For example, the music subscription service Spotify's protocol has been designed to combine server and P2P streaming to decrease the load on Spotify's servers and save bandwith~\cite{Kreitz_undated-yp}, and thus improve the service's scalability. This means that whenever someone listens to a song that another Spotify user is listening to or has recently listened, the Spotify client program may download parts of the song from that user instead of the server. Naturally, P2P streaming comes in handy also whenever there is a short outage, as users might not experience any stoppages to the service, at least when listening to widely streamed content.

The aspects of peer-to-peer networking that have recently gotten attention are cryptocurrency and blockchain technologies, categorized under the umbrella term of distributed ledger technologies. Prior to the emergence of ledger technologies, peer-to-peer systems were often seen in the public light as a technology to work around regulations and for doing lawless activities. This is partly because before their use in blockchain applications, P2P systems were popularized for their use in file sharing applications like Bittorrent. Obviously, this reputation was not earned for nothing, but peer-to-peer networking's use in torrents and other file sharing networks also demonstrates its potential in content delivery networks, or CDNs for short.

Peer-to-peer networks are not without problems. One persistent source of problems is routing and peer discovery. Since the users of a P2P network do not necessarily connect to a centralized and optimized server infrastructure with vast bandwidth and computation resources, the individual connections between peers can be ephemeral. They are usually also random in terms of peer discovery because of the commonly used algorithms and parameters, such as Kademlia used with random peer identifiers. This results in some routes between peers to be inoptimal, and further possibly in bad performance and even false states in distributed ledgers. Slow data propagation in a P2P publish-subscribe network could prevent peers in participating in the consensus due to latency.

Of course, when talking about any networking, latency is usually an issue, but it is particularly so in distributed systems, especially in systems that are designed to reach a consensus. Latency is not only caused by bad network bandwidth and signal loss due to copper cabling that has seen its heyday, but by a number of things ranging from data processing to protocol latency and suboptimal routing. Physical network latency is mostly not a computer science problem, but rather an engineering and materials science problem, as it depends on the physical medium the signal goes through and the distance between the sender and receiver. Processing latency means the latency that is created between the time a peer receives a request and sends a response. This latency is often caused by cryptographic encoding and decoding of messages and fetching data from a database, but in well designed systems this cause of latency is usually negligible, and the problem is largely similar in regular or P2P network applications. Routing latency, however, can be very different between regular networks and P2P networks. Routing in the Internet is handled by familiar devices, routers. Even in P2P overlay networks routing is handled by the regular Internet building blocks like routers, as by definition overlay networks are abstractions on top of existing networks. Overlay P2P networks use IP routing because they use the Internet, but P2P networks need peer discovery since not every IP on the Internet participates in every P2P network available. There needs to be a way to tell which computers with which identifiers and with which IP addresses are a part of a P2P network, and a way of maintaining this info. Routing latency is created when data is routed between peers on the Internet or on a P2P network through relaying peers or slower routes than the global optimum.

The search for algorithmic solutions to routing latency has long been a part of P2P networks research. Since latency between peers in overlay P2P networks is a problem of the network topology, the solutions try to optimize the peer discovery mechanism. Currently, advanced P2P networks usually use, in practice because of public key identifiers, randomized peer discovery, and a high number of concurrent connections resulting in P2P networks with good peer discoverability and resource availability, but unfortunately with subpar routes between peers that are not directly connected to each other. Randomized peer discovery might help with message propagation, as messages can reach far-away peers with fewer hops. For this reason, it might be useful to have a minimal amount of active connections to far-away peers at all times, while still optimizing for individual connections that are close-by. For applications like blockchains this has mostly been ''good enough'', but P2P routing needs to improve with the applications it supports, or else it could become a bottleneck, as block times decrease and blockchain performance increases at large, and P2P utilization grows. In this thesis, I propose a protocol for minimizing routing latency by measuring physical latency using processing latency caused by predefined hard mathematical problems, and saving these measurements in a manner that prevents spoofing.

\section{Contribution}
%TODO: Short intro with better wording what problem I'm trying to solve
Imagine a situation where a peer told you its individual latencies to other peers on its routing table. Couldn't this help with constructing an optimal distributed routing table, with peers only connecting to other peers if they are actually close-by or behind a fast connection?

Distance bounding protocols have tried, and to a certain extent, succeeded in creating secure protocols for measuring the latency between two peers that do not have to trust each other. They are a category of interactive protocols that are used when a simple ping will not suffice, as a prover could craft a distance fraud attack by responding with a pong before seeing the ping message. This type of fraud can lead to problems beyond inoptimal routing, like relay attacks. For example, an attacker could relay a signal from a car to a car key and vice versa, if the access control protocol is not relay attack resistant. Because contactless payments are becoming more and more common all over the world, the problem of relay attacks is highly relevant. To the author's knowledge, there are no other protocols in use today that could provide proofs of a close distance without a trusted verifier, i.e. that a third party could also trust the close distance between the prover and the verifier in a decentralized and trustless setting.

Now, if a third party could trust the given latency between the responder and a randomly selected peer, it could estimate the network topology and find the peers closest to it with a roughly optimal amount of queries, and the P2P network could converge towards an optimal topology. Such a proof of latency could also be used to prove a geographical location, which could be used to battle GPS spoofing. Usually, GPS is a user-facing technology and an individual tool for pathfinding, with only the user requiring the coordinates to be correct. Some applications might require some sort of a proof of a physical location, and this can be done in a centralized manner with bluetooth beacons or with something resembling COVID-19 contact tracing. In a trustless P2P setting, however, this has not been as easily achievable without introducing an incentive structure or requiring the use of trusted computation modules.

% TODO: Explain shortly what VCs and VDFs are before this part here
Using vector commitments plus two concurrent verifiable delay functions and a race between them, I propose a novel protocol for producing a publicly verifiable proof of network latency and difference in computation resources between two participants. The latency is denoted in the difference of iterations between the two verifiable delay functions. This proof can be used for dynamic routing to achieve better, faster routes between peers, or to prove a geographical location.

The protocol's security ultimately depends on each peer's ability to measure against a committed processing power metric of another peer based on their communication. If used with parameters described in this thesis, also all assumptions related to Catalano-Fiore vector commitments~\cite{Catalano2013-jn} and the RSA cryptosystem~\cite{Rivest1978-fm} must hold for the proofs to be valid.

\section{Related Work}
Multiple protocols have aimed at creating a reliable proof of location or latency with varied scopes in the past, starting with distance bounding protocols as a way to measure latency between two computers that don't have to trust each other~\cite{Brands1994-iy}. The following are protocols that demand a mention, as they have tried to solve the same problem as Proof of Latency using different methods.

\subsection{FOAM}
FOAM~\cite{Foamspace_Corp2018-me} is an ongoing open source project that provides decentralized location services with a crowdsourced map that has been incentivized by using their own token on the Ethereum blockchain. Their motivation is that GPS is spoofable and largely unprotected against attacks but trusted by a wide array of critical infrastructure. The proofs of location are created in a web of trust manner, with compounding effects involving trusted peers, or so-called beacons. The proofs consist of a coordinate of their own format combined with a geohash~\footnote{A coordinate that is aunique identifier for a location on Earth} and the Ethereum address of the prover. These proofs are then saved on the Ethereum blockchain, but they can also be checked off-chain without requiring the verifier to use Ethereum.

\subsection{GoAT}
GoAT is a protocol that does file geolocation via anchor timestamping~\cite{Maram_undated-it}. Although the imagined use case is different from Proof of Latency or FOAM, the protocol creates a similar proof to Proof of Latency by measuring an upper bound for latency between two peers that holds even for a third party verifier. GoAT uses timestamping services to enforce causality and to create a timeframe during which the proof must have been created. The resulting proof is meant to be used in a decentralized content delivery network context to ensure that a file has been correctly spread across the network for better availability and performance.

\subsection{Routing}
% \cite{Polymorph}
% TODO: Just move this under P2P background? Tell about multi-level DHT routing etc
