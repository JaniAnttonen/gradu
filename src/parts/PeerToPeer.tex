% !TeX root = ../index.tex
\section{Peer-to-Peer Networking}
Peer-to-peer networking, P2P in short, means that two or more devices communicate with each other with both serving as a client and a server simultaneously. P2P networks vary in scope a lot. Some networks are global, while some are as small, a couple of devices in a local area network, which is the case in printer sharing and some IoT installations. P2P networks can be standalone or rely on some existing infrastructure, like the Internet. The P2P networks that rely on preexisting infrastructure are called overlay networks.

While overlay networks' addressing and routing is usually based on TCP/IP\footnote{Transmission Control Protocol (transport layer) / Internet Protocol (Internet layer), a connection-oriented protocol stack} or UDP/IP\footnote{User Datagram Protocol (transport layer) / Internet Protocol (Internet layer), a connectionless protocol stack}, they must introduce a separate routing scheme, like a distributed hash table or a DHT for short, that works as an index for peers on the network~\cite{Binzenhofer2007-dv}. Distributed hash tables are a key-value mapping of peer identifiers to their addresses, which is stored in a deduplicated manner across peers on a P2P network. Since IP only focuses on addressing the computers and not the applications or resources they offer, it can't function as a peer discovery scheme by itself.

Overlay P2P networks must somehow signal the initial peers, also called bootstrap or introductory peers, to connect to at first. This can be done either by including the bootstrap peers' IP addresses in the application code itself or by providing the same information on a regular web page. The bootstrap peers can be also signaled by word-of-mouth between a group of people, or with wireless broadcast, which is discussed briefly in the next section. This said, usually every peer can function as a bootstrap peer, so knowing the connection information of a single peer on the network can potentially introduce a node into the network at large.

A simple way to create a P2P network would be to share a file containing all the IP adresses a given node has ever connected with with anyone who connects to that node. This is an example of an unstructured P2P network, which could work relatively well in a closed setting, like inside a LAN without support for multicast DNS\footnote{A protocol that enables broadcasting all addresses in a local area network for easy discovery. Used commonly for discovering printers and such.}, or mDNS in short.

\begin{figure}
	\includegraphics[width=\textwidth]{pictures/unstructured.png}
	\caption{An example topography of an unstructured overlay network.}
	\label{Unstructured Overlay Network}
\end{figure}

In contrast to unstructured networks, where peers are connected to at random, structured networks use some logic to form a structured overlay network, rather than just connecting to peers as they come.

\begin{figure}
	\includegraphics[width=\textwidth]{pictures/structured.png}
	\caption{An example topography of a structured overlay network.}
	\label{Structured Overlay Network}
\end{figure}

\section{Role of Latency in Distributed Systems}
It is hardly a surprise, but latency is an important consideration in distributed systems, especially trustless, decentralized ones. Latency is first constrained by the speed of light, and then by hardware and software along the way. In 2012, the global average round-trip delay time to Google's servers was around 100ms.~\cite{Grigorik_undated-mc}

In the new space age the maximum possible latency grows very fast, as there could be peers joining to a distributed network from other planets, space ships or stations. This might seem unnecessary to think about in the distributed P2P context for now, but before all that, we have global satellite mesh Internet providers, like Starlink. Elon Musk, the founder of SpaceX, which deploys the Starlink network of satellites, claims that there is going to be a round-trip\footnote{Including the user's initial request and received response} latency of about 20 milliseconds between a single satellite and the user~\cite{Tung_undated-ny}. In legacy satellite Internet access, the round-trip time even in perfect conditions is about 550 milliseconds~\cite{noauthor_undated-zc}. This difference between legacy and newer satellite Internet comes from the difference in the satellites' orbits and the sheer amount of satellites involved. Legacy satellite Internet uses geostationary orbits, which are very high. These satellites beam on a single face of the earth at a time with limited bandwidth. Newer systems, like Starlink, use a low-earth-orbit, which requires more satellites, since they zoom by at such a speed that constantly changing which satellite one is connected to is a must. The low orbit also means less distance between the satellites and the user. The 20 millisecond latency claimed by Starlink seems like a stretch at first, but is believable when one takes into account that inter-satellite links are done by laser, and light can travel about 31 percent faster in a vacuum than in fiber optics~\cite{Finley2013-wt}. Intercontinental latencies can become much lower because of this.

In blockchains, latency plays a role in the energy efficiency used to achieve consensus. Miners waste energy on a previous block in Proof of Work as long as they have not received information on the winner of the previous block race, as all the miners on the network are trying to find the correct answer to a puzzle at the same time. Thus, it results in energy wasted if miners drag behind the latest block, still trying to solve the previous one. Simulations have shown that if one calculates the round-trip time between the peers that are connected to each other and dropping the ones with larger latencies in favor of lower ones, one can achieve 50\% improvement in average latency with 1 to 2 peers connected. When connectedness grows from the degree of just 1-2 peers up to 20 connected peers, the average latency improvements achieved drop to about 20\%~\cite{Bi_undated-is}. When connectedness is large, there are shorter routes simply by chance to peers one is not directly connected to. For example, in a situation like this, publish-subscribe schemes work faster, propagating messages to the whole network more reliably, because there is less relaying happening.

One cannot keep multiplexing connections\footnote{Having multiple concurrent stateful connections.} forever, as there are hardware and software related limitations to the amount of peers, and there is a Goldilock's zone for the most effective amount of connections. With IPFS\footnote{Interplanetary File System}~\cite{Labs_undated-uw}, for example, the protocol has been breaking users' routers~\cite{Whyrusleeping2016-ej} because of the high number of incoming connections that need to be routed through NAT\footnote{Network address translation. Hides the local area network from the Internet under a different subnet address.}.

Network hops in P2P systems are introduced when two peers are not directly connected to each other, but rather through one or many relays. There are network hops that cannot be easily avoided, like the hops between network routers on the Internet. Most of the P2P routing protocols used today are oblivious to the problem of introducing large hops to communications between two peers, trading network performance for network robustness and decentralization. Some DHT-based protocols, like Kademlia~\cite{Petar_Maymounkov2020-sx}, make the assumption that their users have fast Internet access, and minimize the average latency by selecting connected peers basically at random~\cite{Eigenmann2020-zm}.

While the randomness is great for preventing eclipse attacks\footnote{Eclipsing a peer means purposefully advertising it malicious peers that isolate it from the network}~\cite{Cai2015-ra}, they can introduce unnecessary geographical hops between two peers. If two peers are in the same WAN, for example, in Kademlia they might still connect to each other through a network hop going through another continent. This makes routing data between peers inefficient, resulting in preventable lag when communicating between peers that are not directly connected.
Now, if we were to rely on IP address geolocation, we could more efficiently connect to peers that are close-by. This is unfortunately impossible in privacy-oriented P2P networks, like mixnets, which aim to hide as much of the packet routing information as possible by routing individual packets through different peers and hiding IP addresses of two connected peers from each other~\cite{Harry_Halpin_undated-sq}. Obviously, this also has implications on routing, and thus Proof of Latency. Optimizing for privacy makes it difficult, if not impossible, to measure and advertise latencies between peers reliably and correctly, when packet routes are obfuscated.

\subsection{Ad Hoc and Zero Configuration Networking}
Multicast DNS was proposed by Apple in 2013~\cite{Cheshire2013-ja} as a way of discovering peers in a local area network in a zero-configuration manner. It is used today for resource sharing, such as sharing printers. Multicast DNS does not work outside local area networks, since it works by associating names with IP addresses, like regular DNS does. The problem is that these names are not quaranteed to be unique, and they can therefore be spoofed. If there are two clients with the same name, the first one to respond with its IP address to a query wins~\cite{Pdp2008-tg}. The security of zero-configuration and ad hoc networking must rely on cryptographic identities, so that a peer can verify itself with public-key cryptography. This makes the peers on the network practically unique and thus hard to spoof.

Zero-configuration networking in an unconstrained, global setting is possible with radios, using either dedicated meshnet radios like GoTenna~\cite{GoTenna_undated-km} or Helium~\cite{Helium_undated-jv}, or by using antennas inside mobile devices to form a network. These types of networks are usually called meshnets or ad hoc networks. Walkie-Talkies are the simplest form of a P2P network. Problems can arise due to geography blocking signals, or when one wants to cross large distances with low transmitting capability. Mesh networking has been used most famously in protests worldwide by using a smartphone app called Firechat~\cite{Milian2014-mt}.

Ad hoc mesh networks have a natural metric for latency: signal strength. They can rely on Bluetooth RSSI\footnote{Received Signal Strength Indication}, or triangulate distances by cooperating with multiple peers. These methods are used to locate emergency calls and in contact tracing~\cite{Biddle2020-kl}. Mesh networks, while being peer to peer and not relying on existing infrastructure like overlay networks, still need routing and multiple hops if one wants to reach peers that are not in the operating range of the communication method used. Even ad hoc networks could thus benefit from using Proof of Latency, although the proofs should be updated way more frequently because of the network's mobile nature.

\subsection{Distributed Hash Tables}
Distributed hash tables (DHT) are a way of addressing content to peers in a distributed network. In addition to indexing content in content-addressed networks like IPFS~\cite{Labs_undated-uw}, they can function as routing tables, and have been developed to remove bottlenecks in peer search.

A hash table is a key-value mapping from a to b. What makes a hash table distributed is the fact that the data stored is meant to be distributed between peers, with no single peer keeping all the available data in its hash table, but relaying queries for resources it does not have to other peers on the network. There are multiple versions of DHTs with different methods for prioritizing certain peers: using tree structures, sorting by identifiers, using computational trust, et cetera.

In addition to identifier closeness, DHTs can force a certain network behavior by peer scoring and constructing a web of trust. For example, a peer could only advertise peers that have been connected over a period of time, or enforce reconnecting to disconnected peers that have a good reputation. A widely used trust system is TrustGuard, implemented in the blockchain framework Tendermint.~\cite{Srivatsa2005-ib, Jeff_Foley2018-zt}

Most of the DHT algorithms were invented in the early 2000s, with Kademlia being one of them. DHTs mostly differ just by how distance between peers is defined, and how neighbors are chosen.~\cite{Cai2015-ra}

\subsubsection{Kademlia}
Kademlia is a DHT designed by Petar Maymounkov and David Mazières in 2002~\cite{Petar_Maymounkov2020}. It is based on a tree of identifiers that are split across peers on a network. The identifiers are 160 bits, e.g., a SHA-1 hash of some larger data. Kademlia tries to improve upon previous DHT-based routing algorithms by introducing a symmetric XOR metric for distance between node IDs in the key space~\cite{Petar_Maymounkov2020-sx}. These IDs are sorted in a binary search tree, with each node's position determined by the shortest unique prefix of its ID, like shown in the diagram \ref{Structured Overlay Network} on page \pageref{Structured Overlay Network}. Kademlia makes sure that any node in the network can locate any other node by its ID by ensuring that each node knows at least one of the nodes in each subtree.

A single query in Kademlia has been shown in real-world tests to result in an average of three network hops, meaning that the query gets relayed through two peers before reaching the requested resource~\cite{Roos2013-mb}. Network hops are a necessary evil in distributed systems, and Kademlia does well in requiring on average \(log(n)\) queries in a network of \(n\) nodes. Since the closeness metric is based on a similarity search rather than a measurement, the closest peer is only closest by the identifier, not by network latency.~\cite{Eigenmann2020-zm}

The randomness of Kademlia is great at averaging the network hops required to reach a scarce resource. While this is great for network security, the downside is that it also averages latency, reducing overall performance of the network.

Kademlia protocol has four remote procedure calls, or RPCs in short. These are \(\mathit{PING}\), \(\mathit{STORE}\), \(\mathit{FIND\_NODE}\), and \(\mathit{FIND\_VALUE}\). A Kademlia participant's most important operation is node lookup, i.e., locating \(k\) closest nodes to a given node ID. It is a recursive operation, which starts by picking \(\alpha\) closest nodes from the closest non-empty bucket, and sending them all \(\mathit{FIND\_NODE}\) calls. This is repeated until the initiator has queried and received responses from all \(k\) closest nodes it has seen.

\subsubsection{Polymorph and Network Topologies}
Many DHT networks use what is called a uniring topology. This means that the addressing of the peers is one-dimensional, and do not form logical clusters on the network graph. Unlike Kademlia or other uniring networks like Chord or Tapestry, Polymorph~\cite{Jenkov_undated-kl} is a DHT network that uses a polyring network topology.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{pictures/polyring.png}
	\caption{Polyring network topology.\cite{Jenkov_undated-kl}}
	\label{Polyring}
	\includegraphics[width=0.5\textwidth]{pictures/uniring.png}
	\caption{Uniring network topology.\cite{Jenkov_undated-kl}}
	\label{Uniring}
\end{figure}

Polyring network topologies can use two or more dimensions for their addressing. For example, first a cluster ID and then the peer's local ID in the cluster. This can be used to group peers in the network by geographical location, latency, or application specific heuristics like subscribed channels in a publish subscribe system, et cetera.

Uniring network topologies can result in worse global performance optimum, but they are less susceptible to congestion than polyring network topologies, which have a small number of connections between clusters.~\cite{Jenkov_undated-kl}

\begin{figure}
	\includegraphics[width=\textwidth]{pictures/geographical-polyring.png}
	\caption{Geographically clustered polyring topology.\cite{Jenkov_undated-kl}}
	\label{Geographical Polyring}
\end{figure}


\subsection{Eclipse Attacks}
Although DHTs are most widely used with random hashed identifiers, the distance metric stays the same. By forging identities that are close-by, one can advertise false friends which take over the search space. For example, in a 2019 paper "Eclipsing Ethereum Peers with False Friends" by S. Henningsen, D. Teunis, M. Florian, and B. Scheuermann, the authors demonstrate that to eclipse a victim in Ethereum P2P network, the attacker needs to fill its eight slots for outbound connections, and fill seventeen slots for inbound connections to completely deny service, without going through the attacker's nodes.~\cite{Henningsen2019-mf}

Less structured, random ways of forming connections in an overlay network, like Kademlia, protect against eclipse attacks quite well because of the high connectivity and low locality. Introducing peer scoring and making the network topology more structured opens up possibilities for an attacker to game the scoring system, making sure the target connects to lots of malicious peers eclipsing it. Thus, protecting against eclipse attacks is a balancing act that requires an overlay network to retain some elements of an unstructured network while improving general performance with a structured group of peers.~\cite{Mao2020-ee} This balancing act is seen as a multi-armed bandit problem of exploration and exploitation in the 2020 paper "Perigee: Efficient Peer-to-Peer Network Design for Blockchains" by Y. Mao, S. Deb, S.B. Venkatakrishnan, S. Kannan, and K. Srinivasan.

\subsection{Sybil Attacks}
Sybil attacks mean creating multiple false identities in a P2P network to achieve a botnet-like effect. This could result in gaining disproportionate influence in decentralized governance, block voting, block producing et cetera. An eclipse attack usually involves a sybil attack to create the key pairs that are used to eclipse peers with. Due to peer discovery protocols like the one used in Kademlia being based on public key similarity, creating sybil peers that can eclipse the peer is harder than it sounds like. Although creating new cryptographic identities is not hard, creating identities that are close by is hard, equating to running a Proof of Work algorithm, as outputs in a group of unknown order like RSA are highly random.~\cite{Urdaneta2011-oi, Cholez2009-po}
