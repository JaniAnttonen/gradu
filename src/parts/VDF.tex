% !TeX root = ../index.tex
\section{Verifiable Delay Functions}

A verifiable delay function is a function that calculates sequential calculations that cannot be skipped or significantly sped up by parallellisation, and creates a proof of the calculations that is faster to verify than the calculations are to fully re-evaluate. The computation of a VDF can be split into three distinct parts: Evaluation, proof generation, and verification. The evaluation is the hard sequential calculation from which a proof is generated.~\cite{Boneh2018-sm}

The common parameters in VDF calculation are a random input \(x\), the hash of \(x\) which is called the \(generator\), the group \(G\), and the number of sequential operations, also called the difficulty, \(T\). Every sequential operation during the evaluation consists of an exponentiation and a modulo operation. The exponentiation is always a squaring, so the exponent does not change. This has to do with the proof generation.

Verifiable delay functions are a peculiar bunch of cryptographic proofs, since they are not proving that something has happened or something has been seen, but that a certain amount of time has been spent to compute the result. Like other cryptographic proofs, for the proof to be publicly verifiable it must at least in part be based on public parameters. If a VDF is going to be calculated between two parties, these parties decide the starting parameters between themselves, in a way giving the parameters to each other. This is to prevent possible precomputation --- meaning that the peer that provides a VDF proof could have computed it earlier, separately from the current transaction. If a participant or participants can decide the parameters of the computation by themselves, the computation could have occurred far back in the past.

There's no known way to reduce computation costs sublinearly to the bit length of the exponent used.~\cite{Boneh2018-sm} This means that there's no algorithm-based solution to make VDF computation faster, but there is a growing effort in developing faster hardware for VDF evaluation, especially for doing repeated modular squarings, which would speed up both the evaluation and the proof phases of a VDF.~\cite{noauthor_undated-jb, noauthor_undated-da, noauthor_2020-ki}
% This is a bit abrupt, add info on the different requirements of different parts of VDF calculation

In 2018, two research papers were released independently with descriptions of VDF candidates.~\cite{Wesolowski2018-rf, Pietrzak2018-xs} Before that, also in 2018, a paper formalizing VDFs was published by Boneh, Bonneau, BÃ¼nz, and Fisch, mentioning the term for the first time. There are multiple formulations of a VDF, and not all use a generated proof, instead using parallel processing with graphics processors to check that the delay function has been calculated correctly.~\cite{Yakovenko2018-zn} This bars less powerful devices, like embedded devices, from verifying the VDF's result efficiently. Thus, generating a proof that requires little time to verify is more ideal.~\cite{Boneh_undated-ml}

\subsection{Use Cases}
VDFs could find uses in many things not directly related to blockchains or distributed ledger technologies, and my motivation for this thesis is to find such new use cases. I believe I have found one, using parallel calculations between two peers to measure latency with iterations of this sequential computation.

While Proof of Work is the most widely used consensus algorithm in public distributed ledgers, its use can result in wasteful use of computation resources, especially when there are significant latencies between the participating peers, as discussed in section 2.3. While many other consensus algorithms have been proposed and even applied to distributed ledgers, they come with their own set of problems to tackle. In Proof of Stake network nodes participate in the voting of new blocks by staking a part of their assets as a pawn. Concretely this means that a peer hands the control of some of its cryptocurrency to the consensus algorithm. If a voter gets labeled as malicious, faulty, or absent by a certain majority, its stake can get slashed, losing all or a part of the staked asset in the process. This serves as an incentive for honest co-operation, with sufficient computation resources.

A motivation for VDFs came from a problem with Proof of Stake: block generation votes are not done globally, but by a selected group of peers called the validators, which vote for the contents of proposed blocks that are generated by just one peer at a time, selected as the block generator. The validators are usually selected randomly. This generated a demand for verifiable public randomness that is pre-image resistant, meaning the output of the algorithm generating the randomness cannot be influenced before evaluation by input. This created a need for an algorithm that would prevent multiple malicious actors from being selected to vote at once. Verifiable delay functions can help in creation of public randomness simply because they produce quickly verifiable proofs for results that have high entropy due to repeated calculations with a pre-image resistant hash function.

The hash function inside a VDF can be changed, and VDFs' use in such distributed random beacons is only a small piece of a larger puzzle, as good randomness is hard to achieve using only pseudo-random hash functions. Another problem that served as a motivation for VDFs is scalability and data integrity in distributed ledgers. If the participants of a distributed network of nodes have the same initial input for a high-frequency hash function, and they have similar hardware, it can help putting transactions in order without explicit communication between peers, since you can index each transaction with a hash before communicating it to other peers. The resulting order can be incorrect, but like the case with other forms of synchronized distributed clocks, the main objective is to have an explicit order without ambiguity. This also ties into CRDTs\footnote{Conflict-free Replicated Data Type}, but since that is not the main subject of the thesis, I will not address it.

Time-lock puzzles are precursors to VDFs. Their evaluation can be similar or even identical to VDFs, but they do not contain a proof that is faster to calculate than to re-evaluate the whole puzzle. A way to verify a time-lock puzzle's evaluation would be to link each iteration of the evaluation to the previous iteration, and supply each step of the calculation to the proof's verifier. Then the verifier can use parallel computation to verify the evaluation faster than re-evaluating the whole thing. A puzzle like this is categorized as a proto-VDF.

Many have shown that there are more use cases for verifiable delay functions than puzzles and random number generators. Some examples include preventing front running in P2P cryptocurrency exchanges, spam prevention and rate limiting.~\cite{noauthor_undated-hk} Almost all use cases are based on the property that with a new unique input, and a difficulty requirement, there is no easy way to speed up the calculation of a VDF.

In the front-running use case, this property can be used to restrict front-running in both centralized and decentralized exchanges.~\cite{Khalil2019-sl} Front-running is a term used in both stock trading and cryptocurrency trading which means using inside knowledge, or just a faster connection, of a future transaction to trade an asset with a better price or deny an other transaction from happening.~\cite{Robinson2020-ve} The issue is pronounced in trustless P2P networks like the Ethereum blockchain, where pending transactions are for all to see and can be overtook by promising the network validators a bigger reward, since there's no synchronization before consensus is achieved.~\cite{Mitchell2020-hn} In a centralized exchange, preventing front-running with a VDF means that the exchange waits for a specified time before it starts fulfilling a trade. The VDF serves as a receipt for the order taker that there was no reordering and that take orders were processed in a FIFO\footnote{First in, first out} fashion.~\cite{Cline2020-wb} In a decentralized exchange the same holds if there exists only one peer that can fulfill the order. The peer then functions exactly like the centralized exchange mentioned before.

Spam prevention and rate limiting in the context of VDFs mean roughly the same thing. This is because spam prevention with VDFs is done with rate limiting. If a network application requires peers to perform some sort of Proof of Work in a form of a VDF, by defining the difficulty parameter \(T\) they can set a time boundary for subsequent requests, also called a rate limit. If all the peers need to perform a challenge to request resources from the requestee, a performance-based limit is created for the time between each individual request, thus limiting spam. In a similar fashion, the sequential nature of VDFs can be used to make more power-efficient proof-of-work algorithms, by replacing the requirement from finding a solution to a random challenge to a predetermined challenge with an exact number of steps. This results in better power efficiency, because parallellism will not help in solving the challenge, thus limiting the use of power hungry GPUs or highly parallel ASIC chips for the job.

% TODO: Proofs of Sequential Work or PoSW

%TODO add an intro on VDFs from the boneh paper
\subsection{Precursors to VDFs}
Verifiable delay functions are based on time-lock puzzles. Like VDF's, time-lock puzzles are computational sequential puzzles that require a certain amount of time to solve.\cite{Rivest_undated-qr} Time-lock puzzles were originally created to encrypt information in a manner that could only be opened after a certain amount of computation. Functionally, this is a cryptographic time capsule. Ronald L. Rivest, Adi Shamir, and David A. Wagner classify time-lock puzzles generally under the term \emph{timed-release crypto} in their 1996 paper.~\cite{Rivest_undated-qr} They describe the envisioned use cases for timed-release crypto as being the following:

\begin{itemize}
	\item A bidder in an auction wants to seal his bid so that it can only be opened after the bidding period is closed.
	\item A homeowner wants to give his mortgage holder a series of encrypted mortgage payments. These might be encrypted digital cash with different decryption dates so that one payment becomes decryptable and thus usable by the bank at the beginning of each successive month.
	\item An individual wants to encrypt his diaries so that they are only decryptable after fifty years.
	\item A key escrow scheme can be based on timed-release crypto so that the government can get the message keys but only after a fixed period, say one year.
\end{itemize}

Verifiable delay functions are time-lock puzzles extended with a publicly verifiable proof that is much faster to verify than the puzzle was to solve. The original motivation for verifiable delay functions was to enable someone to pick off where somebody else had left when calculating a time puzzle --- to make it possible to create checkpoints in the calculation that could be trusted. Around the same time, VDFs found use in blockchain applications as a way to achieve unspoofable randomness in elections and smart contract input.

\subsection{Variations}
There are multiple variations on the security principles used within VDFs, since they can use any group of unknown order as a basis for their unpredictability. Known solutions use RSA, elliptic curves and class groups as their cryptographic basis.

Since this work uses the Wesolowski proof in Proof of Latency, I will concentrate on it the most here, but will also explain differences between the most known variations. For the group of unknown order I will use the RSA group in the examples.

\subsubsection{Wesolowski's Efficient Verifiable Delay Function}
The "Efficient" in the name of this VDF protocol by Benjamin Wesolowski means that the proof it generates is efficient to verify by a third party, requiring \( \log _{2} T \) multiplications, with \( T \) being the total amount of modular multiplications. The following is a description of the non-interactive\footnote{Requiring no messages to be sent --- no interaction} variant of the protocol and its phases.

% TODO: Fix long variable names' fonts
\( let \: N = RSA-2048, \mathbb{G} \; = \: multiplicative \: group \: of \: integers \: modulo \: N \)

\( let \: x \: = random \: input,  \: H \: = hash \: function, \text{for example} \: BLAKE3 \)

\( let \: generator \: g = H(x) \to \mathbb{G}, T = difficulty \)

\begin{enumerate}
	\item{Evaluation}

          To evaluate the VDF, one must calculate \(T\) repeated modular squarings. This means raising the generator $g$ to the power of 2 and taking the remainder of that result divided by $N$, $T$ times.

	      As an equation the evaluation output is the following:

	      \( output = g^{2^{T} \mod N } \)

	\item{Proof Generation}

	      \( let \: L = random \: prime \: number \)

	      \( let \: q \in \mathbb{Z}, q = 2^T/L, \: \text{(quotient)} \)

	      \( let \: r \in \mathbb{Z}, r = 2^T\mod L, \: \text{(remainder)} \)

	      \( let \: proof = g^q \)

	      Now that the proof has been calculated, a verifier needs $N$, $output$, $g$, $L$, $r$ and $proof$ to verify that the proof is correct and the prover has actually calculated $g^{2^{T} \mod N }$.

	\item{Verification}

	      To be sure that the prover has calculated the output correctly, a verifier must check the following equation:

	      \( output = proof^L * g^r \:, where \: r = 2^T \mod L \)

	      If the two sides are equal, the VDF has been verified as being correct.
\end{enumerate}

% TODO: Add a simple example of the calculations happening

Different VDF constructs, like the most well known ones by Wesolowski~\cite{Wesolowski2018} and Pietrzak~\cite{Pietrzak2018} make computational trade offs on different parts of the proof. In a Wesolowski VDF the proof generation is a significant part of the whole VDF calculation, comparable in time to the evaluation unless it is optimized in some way. A Pietrzak VDF is faster to prove, but a lot slower to verify. A Wesolowski VDF produces a proof that can be verified in O(1) time.

\subsection{Constructs Related to VDFs}
\subsubsection{Proto-VDF}
Time-lock puzzles that do not include a proof but nevertheless can be verified faster than re-evaluated can be categorized as proto-VDFs. A non-verifiable delay function or a proto-VDF can be verified by using multiple CPU cores or highly parallel graphics processing units, checking each iteration separately with parallellism, like in Solana.~\cite{Yakovenko2018-zn} One could also argue that all blockchains and some random beacons form multi-party calculated VDFs. An algorithm based on sequential hashings, like sha-256 in Solana's Proof of History can be described as a proto-VDF, since it can be verified as correct faster than it can be evaluated with hardware, but doesn't have a separate proof.

% TODO: Add a citation/backlink to a definition of a VDF
\subsubsection{Classic Slow Functions}
Since a VDF's idea is that a hard and slow calculation can be verified to have been performed correctly quickly, any calculation of whose inverse is harder is a candidate for a VDF-type construct. One example of these asymmetric calculations is Sloth.~\cite{Boneh2018-sm} Sloth doesn't include a proof and is not asymptotically verifiable, thus it does not satisfy the definition of a VDF per se.\footnote{Asymptotic, meaning that there is a maximum verification time and the verification can't scale beyond that point.} The evaluation of Sloth is calculated with repeated cubic roots, and it is verified with repeated cubings, with cubic roots being harder to compute than the cubings.

\subsection{Hardware Developments}
VDF applications can be made faster with hardware, and it has been estimated that with an ASIC\footnote{Application-Specific Integrated Circuit. A purpose-built chip for a single algorithm.} chip a VDF can be evaluated more than ten times faster than with a GPU.~\cite{Stanford_Video2020-ap}
The three different parts of a VDF calculation are different in terms of the hardware that can be utilized to make them faster. The evaluation part currently has no de facto hardware to make it faster, but the ASIC and FPGA developments are the ones pushing this part forward. Proving can be made faster with GPUs, whilst verification benefits from fast general computation, thus CPUs.\cite{Protocol_Labs_Kelly_Olson2020-au}

If, or when, hardware specifically optimized for sequential squarings is commercialized, VDFs can become much more mainstream, because they would suffer less from computational differences, thus requiring less trust between the calculating parties. It's a race against time, since if an attacker put a considerable amount of resources in the development of an optimized chip themselves, they could potentially compromise the security of some blockchains, due to an assumption that is made generally in all blockchains: when competing chains are proposed as the truth, the one with the most work is regarded as the winner. By creating blocks faster than the majority of the network, one could attack systems that rely on this assumption with a proprietary chip.

Besides the development of an ASIC driven by vdfresearch.org~\cite{noauthor_undated-hk}, there's been development on CPU instructions by Intel, aiming to help the specific operation of modular exponentiation, which is used not only in VDFs, but also in classical RSA, DSA\footnote{Digital signature algorithm}, and DH\footnote{Diffie-Hellman key exchange} algorithms, and homomorphic encryption\footnote{Homomorphically encrypted data is data that can be done calculations with without revealing it's contents to the calculator.}.~\cite{Drucker2019-cx}
