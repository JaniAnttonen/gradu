% !TeX root = ./Thesis.tex
\chapter{Introduction}
\label{Introduction}

Computer applications, whether they are on the public Internet or on a private network, have long preferred a client-server model of communication. In this model, the client application sends requests to the server, and the server responds. Applications are today, however, increasingly moving towards a distributed, peer-to-peer (P2P) networked model, where every peer, whether it is an entire computer or merely a process running on one, serves equally as the both sides of the client-server model. Many uses of P2P do not even get communicated to the end user, because in many cases, P2P technologies serve as better resource allocators rather than as centerpoints of the applications themselves. For example, the music subscription service Spotify's protocol has been designed to combine server and P2P streaming to improve scalability by decreasing the load on Spotify's servers and bandwith resources.~\cite{Kreitz_undated-yp} Naturally, this comes in handy also whenever there would be a small outage, as users necessarily would not notice any stoppages to the service, at least when listening to widely listened content. 

The aspects of of peer-to-peer networking that have recently got attention have been cryptocurrency and blockchain technologies, categorized under the roof term of distributed ledger technology. Peer-to-peer has rarely been seen in the public light as anything more than a technology to work around regulations and for doing lawless activities. This is partly because before its use in blockchain applications, it was popularized for its use in file sharing applications like Bittorrent, which it still sees use for. Obviously, there is truth to that, but if you look underneath the surface, peer-to-peer networking's use in torrents and other file sharing networks demonstrate its applicability in content delivery networks, or CDNs for short. Peer-to-peer networks are not without problems though, and one problem still relevant to their future is routing and peer discovery. Since the users do not only connect to centralized and optimized server infrastructure with vast bandwidth and computation resources, the individual connections between peers can be ephemeral, and routes between peers that are not directly connected are usually random.

Blockchain networks need a way of synchronizing the latest state of the blockchain globally between a number of peers on a P2P network. Since the blockchain data model is sequential and all recorded history is immutable, an algorithm is needed to reach total synchronization between peers. This kind of algorithm is called a consensus algorithm. This problem is not unique to blockchains, it is relevant in distributed databases as well, but public blockchains have raised new issues that have sparked an ongoing development effort for new kinds of consensus algorithms.

Proof of Work is the most used consensus algorithm in public blockchains today, including Bitcoin, of in which whitepaper it was first described in 2008.~\cite{Nakamoto2019-ax} New algorithms have been introduced since to battle Proof of Work's resource intensiveness, including Proof of Stake, which requires network nodes participating in the voting of new blocks to stake a part of their assets as a pawn. Simply this means handing the control of some of the currency owned to the consensus algorithm if the peer wants to participate in the consensus. If a voter gets labeled as malicious, faulty, or absent by a certain majority, it can get slashed, losing all or a part of the staked asset in the process. This serves as an incentive for honest co-operation, with sufficient computation resources.

One problem with Proof of Stake is that the block generation votes are not done globally, but by a selected group of peers called the validators, which vote for the contents of proposed blocks that are generated by just one peer at a time, selected as the block generator. The validators are usually selected randomly. This has generated an increasing demand for verifiable public randomness that is pre-image resistant, meaning the output of the algorithm generating the randomness cannot be influenced before evaluation by input. This created a need for an algorithm that would prevent multiple malicious actors from being selected to vote at once. A cure for this problem is called a verifiable delay function, a VDF in short. Another problem which served as a motivation to find such a function is scalability and data integrity in distributed ledgers. If the participants of a distributed network of nodes have the same initial input for a high-frequency hash function, and they have similar hardware, it can help putting transactions in order without explicit communication between peers, since you can index each transaction with a hash before communicating it to other peers. The resulting order can be incorrect, but like the case with other forms of synchronized distributed clocks, the main objective is to have an explicit order without ambiguity. This also ties into CRDTs\footnote{Conflict-free Replicated Data Type}, but since that is not the main subject of the thesis, I will not address it.

Time-lock puzzles are precursors to VDFs. Their evaluation can be similar or even identical, but they do not contain a proof that is faster to calculate than to re-evaluate the whole puzzle. One example of such proof would be to link each iteration of the puzzle's evaluation to the previous iteration, and supply each step of the calculation to the proof's verifier. Then the verifier can use parallel computation to verify the proof faster than re-evaluating the whole thing. A proof of this sort is categorized as being a proto-VDF, since there are better and more bandwidth-efficient ways to construct a VDF proof, and some could argue the proof is not strictly a calculated one.

In 2018, two research papers were released independently with similar formalizations of a VDF.~\cite{Wesolowski2018-rf, Pietrzak2018-xs} VDF is an algorithm that requires a specified number of sequential steps to evaluate, but produces a proof that can be efficiently and publicly verified.~\cite{Boneh_undated-ml} To achieve pre-image resistance, a VDF is sequential in nature, and cannot be sped up by parallel processing. There are multiple formulations of a VDF, and not all even have a generated proof, instead using parallel processing with graphics processors to check that the delay function has been calculated correctly.~\cite{Yakovenko2018-zn} This bars less powerful devices, like embedded devices, from verifying the VDF's result efficiently. Thus, generating a proof that requires little time to verify is more ideal.~\cite{Boneh_undated-ml}

VDFs could see usage in many things not directly related to blockchains or distributed ledger technologies, and my motivation for this thesis is to find such new use cases. I believe I have found one that is somewhat related to the distributed synchronized clock mentioned earlier, using a similar parallel construct between two peers to measure latency with iterations of a sequential computation. 

% TODO: Elaborate on eclipse attacks, add citations on the matter
Using a verifiable delay function, I propose a novel algorithm for producing a publicly verifiable proof of network latency and difference in computation resources between two participants in a peer-to-peer network. This proof can be used for dynamic routing to achieve better, faster routes between peers, and for making eclipse attacks\footnote{Eclipse attack means polluting the target's routing table restricting the target's access to the rest of the network, which opens up other attack possibilities regarding consensus algorithms.} harder to achieve. 
