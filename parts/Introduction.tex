% !TeX root = ./Thesis.tex
\chapter{Introduction}
\label{Introduction}

Computer applications, whether they are on the public Internet or on a private network, commonly use the client-server model of communication. In this model, the client application sends requests to the server, and the server responds. Applications are today, however, increasingly moving towards a distributed, peer-to-peer (P2P) networked model, where every peer, whether it is an entire computer or merely a process running on one, serves equally as the both sides of the client-server model. Many uses of P2P are not visible to the end user, because in many cases, P2P technologies serve to optimize resource utilization rather than as centerpoints of applications. For example, the music subscription service Spotify's protocol has been designed to combine server and P2P streaming to decrease the load on Spotify's servers and save bandwith, and thus improve the service's scalability.~\cite{Kreitz_undated-yp} This means that whenever someone listens a song that another Spotify user does listen or has recently listened, the Spotify client program can optionally download parts of the song from another user instead of the server. Naturally, P2P streaming comes in handy also whenever there is a short outage, as users might not experience any stoppages to the service, at least when listening to widely streamed content. 

The aspects of peer-to-peer networking that have recently gotten attention are cryptocurrency and blockchain technologies, categorized under the roof term of distributed ledger technologies. Prior to the emergence of ledger technologies, peer-to-peer systems were often seen in the public light as a technology to work around regulations and for doing lawless activities. This is partly because before their use in blockchain applications, P2P systems were popularized for their use in file sharing applications like Bittorrent. Obviously, this reputation was not earned for nothing, but peer-to-peer networking's use in torrents and other file sharing networks also demonstrates its potential in content delivery networks, or CDNs for short. Peer-to-peer networks are not without problems. One persistent source of problems is routing and peer discovery. Since the users of a P2P network do not necessarily connect to a centralized and optimized server infrastructure with vast bandwidth and computation resources, the individual connections between peers can be ephemeral, and are usually random in terms of peer discovery because of the algorithms and parameters used, like Kademlia used with random peer identifiers. This results in some routes between peers to be inoptimal, which could result in bad performance and even false states in distributed ledgers, where slow data propagation in a P2P publish-subscribe network could mean inability to participate in the consensus due to latency.

Blockchain networks need a way of synchronizing the latest state of the blockchain globally between a number of peers on a P2P network. Since the blockchain data model is sequential and all recorded history is immutable, an algorithm is needed to reach total synchronization and agreement between peers. This kind of an algorithm is called a consensus algorithm. Consensus problems are not unique to blockchains, but are also relevant in distributed databases, as different shards of a database could end up in a diverged state if they did not wait for an individual change to propagate between all the shards. Blockchains have raised new issues in consensus algorithms due to attacks from dishonest peers and resource usage, that have sparked an ongoing development effort for new kinds of consensus algorithms.

Proof of Work is the most used consensus algorithm in public blockchains today, including in Bitcoin, where it was first described in 2008.~\cite{Nakamoto2019-ax} In short, in Proof of Work all participants try to guess a correct answer to a cryptographic puzzle as fast as they can, and a correct answer produces a new block. The new block gets sent and propagated to the network, and if enough peers agree that the answer was correct, the guessing game starts over. In Proof of Work, any latency at all results in wasted resources before they receive the latest block, as they are still trying to solve the puzzle before they know the result.

Of course, when talking about any networking, latency is usually an issue. Issues regarding latency are highlighted in distributed systems, especially so in systems that are designed to reach a consensus. Latency is not only caused by bad network bandwidth and signal loss due to copper cabling that has already seen its heyday, but by a number of things ranging from data processing, routing. Physical network latency is mostly not a computer science problem, but rather an engineering and materials science problem, as it depends on the physical medium the signal goes through and the distance between the sender and receiver. Processing latency means the latency that is created between the time a peer receives a request and sends a response. This latency is often caused by cryptographic encoding and decoding of messages and fetching data from a database, but in well designed systems this cause of latency is usually negligible, and the problem is largely similar in regular or P2P network applications. Routing latency, however, can be very different between regular networks and P2P networks. Routing in the internet is handled by familiar devices, routers, and even P2P overlay networks useIP routing, as by definition overlay networks are abstractions on top of existing networks. Overlay P2P networks use IP routing because they use the internet, but P2P networks need peer discovery since not every IP on the internet participates in every P2P network available. There needs to be a way to tell what computers with which identifiers and with which IP addresses are a part of the P2P network, and a way of maintaining this info. Routing latency is created when data is routed between peers on the internet or on a P2P network through more peers or slower routes than the globally optimum route with either the least amount of peers working as relayers of the messages or the most efficient connections.

Search for algorithmic solutions to routing latency has long been a part of P2P networks research. Since latency between peers in P2P networks is a problem of the network topology, the solutions try to optimize the peer discovery mechanism. Currently, advanced P2P networks use randomized peer discovery and a high number of concurrent connections resulting in P2P networks with good peer discoverability and resource availability, but unfortunately with subpar routes between peers that are not directly connected to each other. For applications like blockchains this has mostly been "good enough", but P2P routing needs to improve with the applications it supports, or else it would become a bottleneck. In this thesis, I try to create a tool for minimizing routing latency by measuring physical latency using processing latency. 

If a peer told you its individual latencies to other peers on its routing table, couldn't this help with constructing an optimal distributed routing table, with peers only connecting to other peers if they are actually close-by or behind a fast connection? I think that it could, but since only a certain number of connections can be kept at one time, this would increase the possibility for so-called eclipse attacks. In an eclipse attack, an attacker tries to influence the victim's connections in a way that it would only be connected to peers controlled by the attacker, effectively isolating the victim from honest peers on the network, increasing the possibility for man-in-the-middle attacks regarding the victim's sent requests or received responses. Also, a not-so-malicious peer could try to game the peer discovery algorithm in the pursuit of better connections to itself at the cost of others. Whilst this is not as severe of a problem, it's still undesirable.

Now, what if you didn't have to trust the peer who reports the latencies, and there was no way for the other peer to lie on the subject? Given this, a peer could roughly estimate the network topology and find the peers closest to it with a roughly optimal amount of queries, and the P2P network could converge towards an optimal topology with limited threat of eclipse attacks. Such a proof of latency could also be used to prove a geographical location, which could be used to battle GPS spoofing. Usually, GPS is a user-facing technology and an individual tool for pathfinding, with only the user requiring the coordinates to be correct. Some applications might require some sort of a proof of a physical location, and this can be done in a centralized manner with bluetooth beacons or with something resembling COVID-19 contact tracing. In a trustless P2P setting, however, this is not as easily achievable without introducing an incentive structure or requiring the use of trusted computation modules.  

Using two concurrent verifiable delay functions and a race between them, I propose a novel algorithm for producing a publicly verifiable proof of network latency and difference in computation resources between two participants in a peer-to-peer network. This proof can be used for dynamic routing to achieve better, faster routes between peers, and for making eclipse attacks\footnote{Eclipse attack means polluting the target's routing table restricting the target's access to the rest of the network, which opens up other attack possibilities.} harder to achieve. 
