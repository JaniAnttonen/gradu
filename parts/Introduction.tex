% !TeX root = ./Thesis.tex
\chapter{Introduction}
\label{Introduction}

Computer applications, whether they are on the public Internet or on a private network, commonly use the client-server model of communication. In this model, the client application sends requests to the server, and the server responds. Applications are today, however, increasingly moving towards a distributed, peer-to-peer (P2P) networked model, where every peer, whether it is an entire computer or merely a process running on one, serves equally as the both sides of the client-server model. Many uses of P2P are not visible to the end user, because in many cases, P2P technologies serve to optimize resource utilization rather than as centerpoints of applications. For example, the music subscription service Spotify's protocol has been designed to combine server and P2P streaming to decrease the load on Spotify's servers and save bandwith, and thus improve the service's scalability.~\cite{Kreitz_undated-yp} This means that whenever someone listens a song that another Spotify user does listen or has recently listened, the Spotify client program can optionally download parts of the song from another user instead of the server. Naturally, P2P streaming comes in handy also whenever there is a short outage, as users might not experience any stoppages to the service, at least when listening to widely streamed content.

The aspects of peer-to-peer networking that have recently gotten attention are cryptocurrency and blockchain technologies, categorized under the roof term of distributed ledger technologies. Prior to the emergence of ledger technologies, peer-to-peer systems were often seen in the public light as a technology to work around regulations and for doing lawless activities. This is partly because before their use in blockchain applications, P2P systems were popularized for their use in file sharing applications like Bittorrent. Obviously, this reputation was not earned for nothing, but peer-to-peer networking's use in torrents and other file sharing networks also demonstrates its potential in content delivery networks, or CDNs for short. Peer-to-peer networks are not without problems. One persistent source of problems is routing and peer discovery. Since the users of a P2P network do not necessarily connect to a centralized and optimized server infrastructure with vast bandwidth and computation resources, the individual connections between peers can be ephemeral, and are usually random in terms of peer discovery because of the algorithms and parameters used, like Kademlia used with random peer identifiers. This results in some routes between peers to be inoptimal, which could result in bad performance and even false states in distributed ledgers, where slow data propagation in a P2P publish-subscribe network could mean inability to participate in the consensus due to latency.

Blockchain networks need a way of synchronizing the latest state of the blockchain globally between a number of peers on a P2P network. Since the blockchain data model is sequential and all recorded history is immutable, an algorithm is needed to reach total synchronization and agreement between peers. This kind of an algorithm is called a consensus algorithm. Consensus problems are not unique to blockchains, but are also relevant in distributed databases, as different shards of a database could end up in a diverged state if they did not wait for an individual change to propagate between all the shards. Blockchains have raised new issues in consensus algorithms due to attacks from dishonest peers and resource usage, that have sparked an ongoing development effort for new kinds of consensus algorithms.

Proof of Work is the most used consensus algorithm in public blockchains today, including in Bitcoin, where it was first described in 2008.~\cite{Nakamoto2019-ax} In short, in Proof of Work all participants try to guess a correct answer to a cryptographic puzzle as fast as they can, and a correct answer produces a new block. The new block gets sent and propagated to the network, and if enough peers agree that the answer was correct, the guessing game starts over. In Proof of Work, any latency at all results in wasted resources before they receive the latest block, as they are still trying to solve the puzzle before they know the result. The most well known alternative to Proof of Work is Proof of Stake. Proof of Stake will not result as many wasted resources if the routing latency between peers is suboptimal, but Proof of Stake's voting system must fix the randomized election problem to be a valid contender. In Proof of Stake systems, the block producer slots are predetermined, which means that only one peer at a time should be creating a predetermined amount of blocks in a given time frame. There has been many algorithms that try to create fair block producer elections, but they all usually use some form of a verifiable random function to achieve provable and untamperable randomness. A verifiable random function is a function that's output can be verified to be created by the submitter and with given input. As you might have guessed already, the block producer slots should be assigned as randomly as possible, so that peers can not conspire against the network by creating blocks with devised transactions or front running\footnote{Placing precedent unfairly to transactions that should be not reordered, sort of like cutting in queue.} and then vote falsely. Some Proof of Stake chains use VRF's as their verifiable randomness, while others use a combination of multi-party computation and VDFs. All these engineered solutions demanded a formal mathematical construction of VDFs in 2018.

Of course, when talking about any networking, latency is usually an issue. Issues regarding latency are highlighted in distributed systems, especially so in systems that are designed to reach a consensus. Latency is not only caused by bad network bandwidth and signal loss due to copper cabling that has already seen its heyday, but by a number of things ranging from data processing, routing. Physical network latency is mostly not a computer science problem, but rather an engineering and materials science problem, as it depends on the physical medium the signal goes through and the distance between the sender and receiver. Processing latency means the latency that is created between the time a peer receives a request and sends a response. This latency is often caused by cryptographic encoding and decoding of messages and fetching data from a database, but in well designed systems this cause of latency is usually negligible, and the problem is largely similar in regular or P2P network applications. Routing latency, however, can be very different between regular networks and P2P networks. Routing in the internet is handled by familiar devices, routers, and even P2P overlay networks useIP routing, as by definition overlay networks are abstractions on top of existing networks. Overlay P2P networks use IP routing because they use the internet, but P2P networks need peer discovery since not every IP on the internet participates in every P2P network available. There needs to be a way to tell what computers with which identifiers and with which IP addresses are a part of the P2P network, and a way of maintaining this info. Routing latency is created when data is routed between peers on the internet or on a P2P network through relaying peers or slower routes than the global optimum.

Search for algorithmic solutions to routing latency has long been a part of P2P networks research. Since latency between peers in overlay P2P networks is a problem of the network topology, as  the solutions try to optimize the peer discovery mechanism. Currently, advanced P2P networks usually use, in practice because of public key identifiers, randomized peer discovery, and a high number of concurrent connections resulting in P2P networks with good peer discoverability and resource availability, but unfortunately with subpar routes between peers that are not directly connected to each other. In some cases, this might help with message propagation, as messages can reach far-away peers with fewer hops. For this reason, it might be useful to have a minimal amount of active connections to far-away peers at all times, while still optimizing for individual connections that are close-by. For applications like blockchains this has mostly been "good enough", but P2P routing needs to improve with the applications it supports, or else it could become a bottleneck, as block times and blockchain performance increases at large. In this thesis, I try to create a tool for minimizing routing latency by measuring physical latency using processing latency.

Imagine a situation where a peer told you its individual latencies to other peers on its routing table. Couldn't this help with constructing an optimal distributed routing table, with peers only connecting to other peers if they are actually close-by or behind a fast connection? That could help in constructing an overlay P2P topology that is optimal, but since only a certain number of connections can be kept at one time, this would increase the possibility for so-called eclipse attacks. In an eclipse attack, an attacker tries to influence the victim's connections in a way that it would only be connected to peers controlled by the attacker, effectively isolating the victim from honest peers on the network, increasing the possibility for man-in-the-middle attacks regarding the victim's sent requests or received responses. Also, a not-so-malicious peer could try to game the peer discovery algorithm in the pursuit of better connections to itself at the cost of others. Whilst this is not as severe of a problem, it's still undesirable.

Distance bounding protocols have tried, and to a certain extent, succeeded in creating secure protocols for measuring the latency between two peers that do not have to trust each other. They are a category of interactive protocols that are used when a simple ping will not suffice, as a prover could craft a distance fraud attack by responding with a pong before seeing the ping message. This type of fraud can lead to problems beyond inoptimal routing, like relay attacks. For example, an attacker could relay a signal from a car to a car key and vice versa, if the access control protocol is not relay attack resistant. Also, as contactless payments are becoming more and more common all over the world, the problem of relay attacks is highly relevant. There however are not a lot of protocols in use today that could provide proofs of a close distance without a trusted verifier, meaning that a third party could also trust the close distance between the prover and the verifier in a decentralized and trustless setting.\cite{Maram_undated-it}

Now, what if you didn't have to trust the peer who reports the latencies, and there was no way for the other peer to lie on the subject? Given this, a peer could roughly estimate the network topology and find the peers closest to it with a roughly optimal amount of queries, and the P2P network could converge towards an optimal topology with limited threat of eclipse attacks. Such a proof of latency could also be used to prove a geographical location, which could be used to battle GPS spoofing. Usually, GPS is a user-facing technology and an individual tool for pathfinding, with only the user requiring the coordinates to be correct. Some applications might require some sort of a proof of a physical location, and this can be done in a centralized manner with bluetooth beacons or with something resembling COVID-19 contact tracing. In a trustless P2P setting, however, this has not been as easily achievable without introducing an incentive structure or requiring the use of trusted computation modules.

Using vector commitments plus two concurrent verifiable delay functions and a race between them, I propose a novel algorithm for producing a publicly verifiable proof of network latency and difference in computation resources between two participants in a peer-to-peer network. This proof can be used for dynamic routing to achieve better, faster routes between peers, or to prove a geographical location.
