% !TeX root = ./Thesis.tex
\section{Cryptography}

Cryptography is a field of mathematics that uses computationally difficult problems to obfuscate, hide, split and verify data. Starting in the ancient times with the famous Caesar cipher, which relied on shifting the letters in a message in coordination to an alphabet, there are multiple cryptographic protocols today for a variety of use cases, which are used in both public and private messaging. As mentioned earlier, while the familiar connotation of cryptography as a word brings secrecy and secure communications into mind, it can also be used to verify data, and to prove that something has happened. I will go over cryptography subjects that are relevant to the Proof of Latency protocol in this chapter, like groups of unknown order, RSA and cryptographic proofs.

Modern cryptography is based on relatively few mathematical fundamentals. The most dominant one during the age of computers has been prime numbers with modular multiplication, but elliptic curve cryptography is used widely today because of its easy and less resource-intensive key generation method when compared to the prime number based RSA cryptosystem. They both use modular arithmetic in their operations.

Modular\footnote{Using the modulus operation; taking the remainder of a division. Useful in cryptography because it creates finite cyclical groups.} arithmetic with large numbers has a useful property that you can exchange encrypted data without the participants knowing each other's private keys, but requiring a computation that is theoretically almost impossible to break due to the prime factorization problem. These large numbers that are used as the modulo need to have unknown factorizations to create groups of unknown order, that are paramount to cryptographic systems that rely on the factorization problem for their safety. The order of a group means the amount of elements in it. Cryptographic proofs need to be created with groups of unknown orders for them to be valid, as knowing the order of a group means that the modulo used in the cryptographic operations is so small that it has a known factorization, and the security of the factorization problem is broken.

\subsection{RSA}
RSA is named after it's discoverers â€“ Rivest, Shamir, Adleman. It is an asymmetric public-key cryptosystem, meaning that it is based on a keypair, in which one is a public and the other is a secret. The public key is used to encrypt, and the secret key is used to decrypt. Everyone who has the public key can encrypt data and that encrypted data is practically impossible to decrypt without the secret key. This works due to the fact that it is hard to factor the product of two large prime numbers. RSA has been the most widely used cryptosystem since its creation, and is easy to use in modern context by just increasing the size of the keys.

RSA is an arithmetic trick that creates a mathematical object called a trapdoor permutation. Trapdoor permutation is a function that transforms a number x to a number y in the same range, in a way that computing y from x is easy using the public key but computing x from y is practically impossible without knowing the private key. The private key is the trapdoor.~\cite{Aumasson2018-nh} RSA relies on the prime factorization problem and the strong RSA assumption for its security. Strong RSA groups are groups of unknown order, meaning that the number of elements in the group is unknown. RSA has played a big part in the development of cryptography, and even served as a motivation for the creation of verifiable delay functions, as it was used in the creation of time-lock puzzles closely related to VDFs.
% TODO: Not the correct assumptions listed here

\subsubsection{Setup}
At first, the users need to setup the public parameters, which need to be distributed between the involved peers to be used in encryption. This key generation process is quite resource intensive, and is rarely used in ephemeral contexts, unless you can skip the key generation process, or ceremony, and use a publicly available modulo that is created by a trusted party. These types of key generation processes are called trusted setups, and they produce so-called toxic waste that needs to be discarded to preserve the key safety.

The company formed by the RSA algorithm's inventors, RSA Security LLC, published public RSA semiprimes\footnote{A product of two prime numbers} that are called RSA numbers as a part of the RSA factoring challenge from 1991 up until 2007 when the challenge ended.~\cite{RSA_Laboratories2013-zk} The challenge was ended because it had reached its purpose of forwarding science and understanding of common symmetric-key and public-key algorithms. Despite this, still under a half of the RSA numbers have been factored, and the largest of them might take hundreds if not even thousands of years to break even when given extraordinary hardware to do it with.

Using these factoring challenge RSA numbers requires the user to assume that nobody has access to the parameters. Given that the RSA numbers are said to have been created with a machine that was completely destroyed after their creation and the primes were never apparent to anyone, you need to trust the company's claims if you ever use these public challenge numbers. Fortunately, usually they are not used for encrypting large swathes of personal data, but in a more ephemeral context like Proof of Latency, which wouldn't cause a huge stir if the cryptosystem was broken and needed changing.

\subsection{Cryptographic Proofs and Their Soundness}
Cryptographic proofs are proofs that depend on the trapdoor nature of cryptographic functions. In other words, cryptographic proofs are arguments that can be trusted if the cryptographic assumptions they depend on are valid and not broken. The most well-known proofs, which are not necessarily thought of as such, are digital signatures. Given a public key, a prover in possession of its corresponding private key can produce a signature of any given data, given to a verifier together with the original data that the prover has seen and processed the data. If the private key pertaining to the public key gets leaked to a third party, its security is broken, and the unique identity is lost.

Proofs can be private or public. A cryptographic proof can be categorized as public, if a verifier can gather all information required to verify the proof from the transcript of the proof itself, and verify the proof to be correct. Now, since classical cryptography is based on the fact that a computation is asymmetric --- being harder to compute the other way around, a cryptographic proof is still probabilistic in nature, and the security of it based not only on the algorithm but also the parameters used.

Proofs need to be tested for soundness to be called as such. Soundness means that a prover cannot make a verifier accept a false statement with reasonable probability. This means that a proof is dependent on context, the verifier. If a proof holds statistically, it's a proof. If it instead holds only computationally, it should be instead called an argument.


\subsection{Commitment Schemes}
Commitment schemes are a way to lock causality in a cryptographic protocol, an are among the most used primitives in cryptography. Commitments usually come in the form of commit-reveal, when at a later stage of the protocol the committee reveals the original value \(x\) they made their commitment \(C\) on. This has a great property --- a prover cannot change their committed value after revealing their commitment to the verifier, because the commitment would not hold for a new value. Commitment schemes are usually used when an interactive protocol requires for a value to be hidden until the termination of the protocol.

\subsubsection{Vector Commitments}
Vector commitments are a recently formalized primitive consisting of a vector \(V\) that as you guessed it, is filled with commitments. The simplest form of a vector commitment most already know is a merkle tree. Unfortunately, merkle proof size grows with input, but newer forms of vector commitments have been discovered after its formalization in 2013. For example, Catalano-Fiore vector commitments improve upon the proof size, introducing a fixed size proof. In addition to the fixed size, the verifier only needs to know the proof and the digest, or the latest output, to verify the proof. In layman's terms, vector commitments enable a prover to prove that a commitment \(v_n\) has been made at the index \(n\) of vector \(v\).

%TODO: Add a formal definition of vector commitments and regular commitments (use an example like Pedersen commitments)

Like regular commitment schemes, vector commitments are used to enforce causality. While regular commitment schemes make sure that a user of a protocol can't change an input or "skip the line" after the start, vector commitments create a causal relationship for multiple proofs. To wrap it up, if proofs are based on vector commitments, the commitments guarantee that the proofs' order can't change, unless the whole vector with all associated proofs are unvalidated.

A Merkle tree is a data structure and a cryptographic accumulator in which you can traverse to the tree's root from any leaf node on it, as a node is always linked to the previous node by a hash that has been calculated with the hash of the previous node. That's right, a blockchain is a Merkle tree, but with added steps.

