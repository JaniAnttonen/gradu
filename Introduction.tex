% !TeX root = ./Thesis.tex
\chapter{Introduction}
\label{Introduction}

%Kirjallisuusviitteet lisätään bib-muodossa bibliografiatiedostoon ja niihin viitataan niiden ID:llä, joka on bib-muodon ensimmäinen kenttä \cite{crawley2007write}.

\section{Background and Motivation}
\label{Background and Motivation}

Distributed systems are vulnerable to problems related to data integrity, because in many cases there is no single source of truth. Work pioneered by the likes of Leslie Lamport has tried to mitigate these problems with clock synchronization and consensus algorithms, which have fixed some but not all of the underlying issues related to the field.



%TODO: Edelliseen kappaleeseen vähän lisää beeffiä ja järkeä

Proof of Work is the most well-known consensus algorithm, as it is used in most public blockchains today, including Bitcoin, of in which whitepaper it was first described in 2008. New algorithms have been introduced since to battle it's resource intensiveness, including Proof of Stake, which requires network nodes participating in the voting of new blocks to stake a part of their assets as a pawn. If the voters get labeled as malicious by a certain majority, they can get slashed, losing all or a part of the staked asset in the process. This serves as an incentive for honest co-operation, with sufficient computation resources.

Problem is that the block generation votes are not done globally, but by a selected group of peers called the validators, which vote for the contents of proposed blocks by one peer selected as the block generator. The validators are selected randomly. This has generated an increasing demand for verifiable public randomness, that is pre-image resistant, meaning the output of the algorithm generating the randomness cannot be influenced beforehand. This created a motivation for an algorithm that would prevent multiple malicious actors from being selected to vote at once, effectively creating a more secure public blockchain.

In 2018, two research papers were released independently with similar formalizations of a VDF.\cite{wesolowski_efficient_2018}\cite{pietrzak_simple_2018} By definition, a VDF is an algorithm that requires a specified number of sequential steps to evaluate, but produces a unique output that can be efficiently and publicly verified.\cite{boneh_verifiable_2018} To achieve pre-image resistance, a VDF is sequential in nature, and cannot be sped up by parallel processing. There are multiple formulations of a VDF, and not all even have a generated proof, instead using parallel processing with graphics processors to check that the calculation is sequential.\cite{yakovenko_solana_nodate} This bars less powerful devices, like embedded devices, from verifying the result efficiently. Thus, generating an efficient proof that requires little time to verify is more ideal.\cite{boneh_verifiable_2018}

Using verifiable delay functions, I propose a novel algorithm for producing a publicly verifiable proof of network latency and computation resource difference between two participants in a peer-to-peer network. This proof can be used for dynamic routing in peer-to-peer networks to reduce latency between peers, and for making sybil attacks harder to achieve.
